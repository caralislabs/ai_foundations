{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ec89a-90b6-4add-91f0-c3b96f77dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import mlflow.data\n",
    "from datetime import datetime\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "import text_preprocessing as tpp\n",
    "\n",
    "import embeddings as embd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16c96df2-0245-426c-a31d-b6e884f11383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d23b7352-8135-433d-aa05-65183fc311d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f882f096-3f1e-4573-92af-fed29c74d34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nicolae/caralislabs/ml-stuff/my_blog/sentiment_analysis/train.csv'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download latest version of the datasets and copy them to the current folder\n",
    "path = kagglehub.dataset_download(\"abhi8923shriv/sentiment-analysis-dataset\")\n",
    "shutil.copy(f'{path}/test.csv', f'{os.getcwd()}/test.csv')\n",
    "shutil.copy(f'{path}/train.csv', f'{os.getcwd()}/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3ab67eb-8d83-42d1-8280-59babb17dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab_to_idx, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert text to indices\n",
    "        tokens = self.preprocess_text(text)\n",
    "        indices = [self.vocab_to_idx.get(token, self.vocab_to_idx['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        if len(indices) < self.max_length:\n",
    "            indices.extend([self.vocab_to_idx['<PAD>']] * (self.max_length - len(indices)))\n",
    "        else:\n",
    "            indices = indices[:self.max_length]\n",
    "        \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        # Simple text preprocessing\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        tokens = text.split()\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22ac060b-c695-4124-ae2b-f76e71c13a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers=2, dropout=0.3, \n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        if pretrained_embeddings is not None:\n",
    "            print(\"Loading pre-trained embeddings...\")\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "            if freeze_embeddings:\n",
    "                print(\"Embedding weights frozen\")\n",
    "            else:\n",
    "                print(\"Embedding weights will be fine-tuned\")\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # hidden: [num_layers * num_directions, batch, hidden_dim]\n",
    "        # Take the last layer's hidden states for both directions\n",
    "        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)  # [batch, hidden_dim*2]\n",
    "        \n",
    "        output = self.dropout(hidden_cat)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fd64ac8-7e44-4d95-a703-3bcbcefbb3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    now = datetime.now()\n",
    "    run_name = f\"tracking run at: {now}\"     \n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "        # Some searchable tags\n",
    "        experiment_tags = {\n",
    "            \"project_name\": \"sentiment_analysis\",\n",
    "        }\n",
    "        \n",
    "        experiment_id = get_or_create_experiment(experiment_name=\"Sentiment Analysis with pytorch model\", experiment_tags=experiment_tags)\n",
    "        \n",
    "        mlflow.set_experiment(experiment_id=experiment_id)\n",
    "\n",
    "        # Configuration\n",
    "        CONFIG = {\n",
    "            'embedding_dim': 100,\n",
    "            'hidden_dim': 256,\n",
    "            'n_layers': 2,\n",
    "            'dropout': 0.3,\n",
    "            'learning_rate': 0.01,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 10,\n",
    "            'max_length': 128,\n",
    "            'min_freq': 2,\n",
    "            'use_pretrained_embeddings': True,  # Set to False to train from scratch\n",
    "            'freeze_embeddings': True,  # Set to True to freeze pre-trained embeddings\n",
    "            'glove_path': None,  # Set to your GloVe file path, or None to auto-download\n",
    "        }\n",
    "        \n",
    "        # Device configuration\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Load datasets (replace with your actual file paths)\n",
    "        print(\"Loading datasets...\")\n",
    "        train_df = pd.read_csv('train.csv', encoding='latin1')\n",
    "        test_df = pd.read_csv('test.csv', encoding='latin1')\n",
    "        # clean-up the datasets\n",
    "        train_df = train_df[['text', 'sentiment']].dropna()\n",
    "        test_df = test_df[['text', 'sentiment']].dropna()\n",
    "        \n",
    "        print(f\"Training data: {len(train_df)} samples\")\n",
    "        print(f\"Test data: {len(test_df)} samples\")\n",
    "        print(f\"Sentiment distribution in training data:\")\n",
    "        print(train_df['sentiment'].value_counts())\n",
    "        \n",
    "        # Prepare label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        all_sentiments = pd.concat([train_df['sentiment'], test_df['sentiment']])\n",
    "        label_encoder.fit(all_sentiments)\n",
    "        \n",
    "        # Encode labels\n",
    "        train_labels = label_encoder.transform(train_df['sentiment'])\n",
    "        test_labels = label_encoder.transform(test_df['sentiment'])\n",
    "        \n",
    "        print(f\"Label mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "        \n",
    "        # Build vocabulary\n",
    "        print(\"Building vocabulary...\")\n",
    "        vocab_to_idx = tpp.build_vocabulary(train_df['text'], min_freq=CONFIG['min_freq'])\n",
    "\n",
    "        mlflow.log_dict(vocab_to_idx, \"vocab_to_idx\")\n",
    "        \n",
    "        vocab_size = len(vocab_to_idx)\n",
    "        print(f\"Vocabulary size: {vocab_size}\")\n",
    "        \n",
    "        # Load pre-trained embeddings if specified\n",
    "        pretrained_embeddings = None\n",
    "        if CONFIG['use_pretrained_embeddings']:\n",
    "            # Handle GloVe embeddings\n",
    "            glove_path = CONFIG['glove_path']\n",
    "            if glove_path is None:\n",
    "                # Auto-download GloVe embeddings\n",
    "                glove_path = embd.download_glove_embeddings(CONFIG['embedding_dim'])\n",
    "            \n",
    "            # Load GloVe embeddings\n",
    "            glove_embeddings = embd.load_glove_embeddings(glove_path, CONFIG['embedding_dim'])\n",
    "            pretrained_embeddings = embd.create_embedding_matrix(vocab_to_idx, glove_embeddings, CONFIG['embedding_dim'])\n",
    "        \n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = TextDataset(train_df['text'].values, train_labels, vocab_to_idx, CONFIG['max_length'])\n",
    "        test_dataset = TextDataset(test_df['text'].values, test_labels, vocab_to_idx, CONFIG['max_length'])\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        output_dim = len(label_encoder.classes_)\n",
    "        model = SentimentLSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=CONFIG['embedding_dim'],\n",
    "            hidden_dim=CONFIG['hidden_dim'],\n",
    "            output_dim=output_dim,\n",
    "            n_layers=CONFIG['n_layers'],\n",
    "            dropout=CONFIG['dropout'],\n",
    "            pretrained_embeddings=pretrained_embeddings,\n",
    "            freeze_embeddings=CONFIG['freeze_embeddings']\n",
    "        ).to(device)\n",
    "        \n",
    "        print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "        \n",
    "        # Training loop\n",
    "        print(\"\\nStarting training...\")\n",
    "        best_accuracy = 0\n",
    "        \n",
    "    \n",
    "        # Log configuration\n",
    "        mlflow.log_params(CONFIG)\n",
    "\n",
    "        # Log datasets\n",
    "        test_dataset = mlflow.data.from_pandas(df=test_df, name=\"test_dataset\")\n",
    "        train_dataset = mlflow.data.from_pandas(df=train_df, name=\"train_dataset\")        \n",
    "        mlflow.log_input(test_dataset, context=\"test_dataset\")\n",
    "        mlflow.log_input(train_dataset, context=\"train_dataset\")\n",
    "        \n",
    "        for epoch in range(CONFIG['epochs']):\n",
    "            train_loss, train_accuracy = train_model(model, train_loader, criterion, optimizer, device)\n",
    "            test_loss, test_accuracy, _, _ = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_accuracy\", train_accuracy, step=epoch)\n",
    "            mlflow.log_metric(\"test_loss\", test_loss, step=epoch)\n",
    "            mlflow.log_metric(\"test_accuracy\", test_accuracy, step=epoch)            \n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{CONFIG['epochs']}]\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "                # Save best model\n",
    "            if test_accuracy > best_accuracy:\n",
    "                best_accuracy = test_accuracy\n",
    "                best_model_path = \"best_sentiment_model.pth\"\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'vocab_to_idx': vocab_to_idx,\n",
    "                    'label_classes': label_encoder.classes_.tolist(),  # Save classes instead of encoder\n",
    "                    'config': CONFIG\n",
    "                }, 'best_sentiment_model.pth')\n",
    "\n",
    "                # Log model and best accuracy\n",
    "                mlflow.log_metric(\"best_accuracy\", best_accuracy)\n",
    "                # mlflow.log_artifact(best_model_path)\n",
    "                # mlflow.pytorch.log_model(model, artifact_path=\"model\")\n",
    "                log_model(model=model, vocab_to_index=vocab_to_idx,\n",
    "                          vocab_size=vocab_size, max_length=CONFIG['max_length'], device=device)\n",
    "\n",
    "        # Log final metrics\n",
    "        test_loss, test_accuracy, predictions, true_labels = evaluate_model(model, test_loader, criterion, device)\n",
    "        mlflow.log_metric(\"final_test_accuracy\", test_accuracy)                \n",
    "        \n",
    "        # Final evaluation\n",
    "        print(\"\\nFinal Evaluation:\")\n",
    "        test_loss, test_accuracy, predictions, true_labels = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        report = classification_report(true_labels, predictions, target_names=label_encoder.classes_)\n",
    "        print(report)\n",
    "        \n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(true_labels, predictions))\n",
    "        \n",
    "        print(f\"\\nBest model saved as 'best_sentiment_model.pth' with accuracy: {best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68c8659f-a0cf-48a6-b73a-4248ab0018f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model(model, vocab_to_index, vocab_size, max_length, device):\n",
    "    from mlflow.models.signature import infer_signature\n",
    "\n",
    "    sample_input, sample_output = generate_sample_input_output(model, vocab_to_index, vocab_size, max_length, device)\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(sample_input, sample_output)\n",
    "\n",
    "    # Log the model with input example and signature\n",
    "    mlflow.pytorch.log_model(\n",
    "        model,\n",
    "        artifact_path=\"model\",\n",
    "        input_example=sample_input,\n",
    "        signature=signature\n",
    "    )\n",
    "\n",
    "def generate_sample_input_output(model, vocab_to_index, vocab_size, max_length, device):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "\n",
    "    # Create a sample input batch (1 sequence of max_length tokens)\n",
    "    sample_input = np.array(tpp.text_to_input(\"This is the best movie ever\", vocab_to_index))\n",
    "    sample_input_tensor = torch.from_numpy(sample_input).to(device)\n",
    "    \n",
    "    # Run the model to get sample output\n",
    "    sample_output = model(sample_input_tensor)\n",
    "\n",
    "    # Convert input to numpy for signature and input_example\n",
    "    sample_input = sample_input_tensor.cpu().numpy()\n",
    "    sample_output = sample_output.detach().cpu().numpy()\n",
    "    return sample_input, sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cd1e54f-5265-4da6-9865-3bcee402f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(experiment_name: str, experiment_tags: dict) -> str:\n",
    "    # Check if the experiment already exists\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    \n",
    "    if experiment is not None:\n",
    "        print(f\"Using existing experiment: {experiment_name} (ID: {experiment.experiment_id})\")\n",
    "        return experiment.experiment_id\n",
    "    else:\n",
    "        # Create a new experiment\n",
    "        experiment_id = mlflow.create_experiment(name=experiment_name, tags=experiment_tags)\n",
    "        print(f\"Created new experiment: {experiment_name} (ID: {experiment_id})\")\n",
    "        return experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "865f2f5b-55f3-40a4-9f03-aea47c701ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_texts, batch_labels in train_loader:\n",
    "        batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_texts)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(train_loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbecbacb-b195-4218-87c1-53ae53f35537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_texts, batch_labels in test_loader:\n",
    "            batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_texts)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    return total_loss / len(test_loader), accuracy, all_predictions, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22f32ed5-b9ad-4952-af74-998acab6c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model_path='best_sentiment_model.pth'):\n",
    "    \"\"\"Function to predict sentiment of a single text\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load model and components with weights_only=False for backward compatibility\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading with weights_only=False: {e}\")\n",
    "        # Try with weights_only=True and safe globals\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        torch.serialization.add_safe_globals([LabelEncoder])\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    \n",
    "    vocab_to_idx = checkpoint['vocab_to_idx']\n",
    "    config = checkpoint['config']\n",
    "    \n",
    "    # Handle both old and new save formats\n",
    "    if 'label_encoder' in checkpoint:\n",
    "        # Old format with sklearn LabelEncoder\n",
    "        label_classes = checkpoint['label_encoder'].classes_\n",
    "    else:\n",
    "        # New format with classes list\n",
    "        label_classes = checkpoint['label_classes']\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SentimentLSTM(\n",
    "        vocab_size=len(vocab_to_idx),\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=len(label_classes),\n",
    "        n_layers=config['n_layers'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    indices = [vocab_to_idx.get(token, vocab_to_idx['<UNK>']) for token in tokens]\n",
    "    \n",
    "    # Pad or truncate\n",
    "    if len(indices) < config['max_length']:\n",
    "        indices.extend([vocab_to_idx['<PAD>']] * (config['max_length'] - len(indices)))\n",
    "    else:\n",
    "        indices = indices[:config['max_length']]\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([indices], dtype=torch.long).to(device)\n",
    "        output = model(input_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    sentiment = label_classes[predicted_class]\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return sentiment, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c801b3f8-034c-4237-bd3b-24c8f3c0e15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing experiment: Sentiment Analysis with pytorch model (ID: 511091070605373857)\n",
      "Using device: cuda\n",
      "Loading datasets...\n",
      "Training data: 27480 samples\n",
      "Test data: 3534 samples\n",
      "Sentiment distribution in training data:\n",
      "sentiment\n",
      "neutral     11117\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: count, dtype: int64\n",
      "Label mapping: {'negative': np.int64(0), 'neutral': np.int64(1), 'positive': np.int64(2)}\n",
      "Building vocabulary...\n",
      "Vocabulary size: 10096\n",
      "Loading GloVe embeddings from glove.6B.100d.txt...\n",
      "Loaded 400000 word vectors\n",
      "Found pre-trained vectors for 9214/10096 words (91.3%)\n",
      "Loading pre-trained embeddings...\n",
      "Embedding weights frozen\n",
      "Model initialized with 3321283 parameters\n",
      "\n",
      "Starting training...\n",
      "Epoch [1/10]\n",
      "Train Loss: 0.8632, Train Acc: 0.6092\n",
      "Test Loss: 0.7258, Test Acc: 0.6941\n",
      "--------------------------------------------------\n",
      "Using device: cuda\n",
      "Epoch [2/10]\n",
      "Train Loss: 0.7369, Train Acc: 0.6918\n",
      "Test Loss: 0.6986, Test Acc: 0.7100\n",
      "--------------------------------------------------\n",
      "Using device: cuda\n",
      "Epoch [3/10]\n",
      "Train Loss: 0.7166, Train Acc: 0.7047\n",
      "Test Loss: 0.6753, Test Acc: 0.7204\n",
      "--------------------------------------------------\n",
      "Using device: cuda\n",
      "Epoch [4/10]\n",
      "Train Loss: 0.7010, Train Acc: 0.7134\n",
      "Test Loss: 0.6756, Test Acc: 0.7136\n",
      "--------------------------------------------------\n",
      "Epoch [5/10]\n",
      "Train Loss: 0.6823, Train Acc: 0.7205\n",
      "Test Loss: 0.6781, Test Acc: 0.7162\n",
      "--------------------------------------------------\n",
      "Epoch [6/10]\n",
      "Train Loss: 0.6592, Train Acc: 0.7318\n",
      "Test Loss: 0.6537, Test Acc: 0.7320\n",
      "--------------------------------------------------\n",
      "Using device: cuda\n",
      "Epoch [7/10]\n",
      "Train Loss: 0.6590, Train Acc: 0.7328\n",
      "Test Loss: 0.6653, Test Acc: 0.7235\n",
      "--------------------------------------------------\n",
      "Epoch [8/10]\n",
      "Train Loss: 0.6481, Train Acc: 0.7365\n",
      "Test Loss: 0.6648, Test Acc: 0.7210\n",
      "--------------------------------------------------\n",
      "Epoch [9/10]\n",
      "Train Loss: 0.6451, Train Acc: 0.7398\n",
      "Test Loss: 0.6839, Test Acc: 0.7250\n",
      "--------------------------------------------------\n",
      "Epoch [10/10]\n",
      "Train Loss: 0.6402, Train Acc: 0.7378\n",
      "Test Loss: 0.6869, Test Acc: 0.7221\n",
      "--------------------------------------------------\n",
      "\n",
      "Final Evaluation:\n",
      "Test Accuracy: 0.7221\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.60      0.68      1001\n",
      "     neutral       0.63      0.83      0.72      1430\n",
      "    positive       0.86      0.70      0.77      1103\n",
      "\n",
      "    accuracy                           0.72      3534\n",
      "   macro avg       0.76      0.71      0.72      3534\n",
      "weighted avg       0.75      0.72      0.72      3534\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 596  385   20]\n",
      " [ 139 1187  104]\n",
      " [  23  311  769]]\n",
      "\n",
      "Best model saved as 'best_sentiment_model.pth' with accuracy: 0.7320\n",
      "\n",
      "Example predictions:\n",
      "Text: 'I absolutely love this product!'\n",
      "Predicted sentiment: positive (confidence: 0.9444)\n",
      "\n",
      "Text: 'This is the worst thing ever.'\n",
      "Predicted sentiment: negative (confidence: 0.8948)\n",
      "\n",
      "Text: 'It's okay, nothing special.'\n",
      "Predicted sentiment: neutral (confidence: 0.7907)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    # Example usage of prediction function\n",
    "    print(\"\\nExample predictions:\")\n",
    "    example_texts = [\n",
    "        \"I absolutely love this product!\",\n",
    "        \"This is the worst thing ever.\",\n",
    "        \"It's okay, nothing special.\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for text in example_texts:\n",
    "            sentiment, confidence = predict_sentiment(text)\n",
    "            print(f\"Text: '{text}'\")\n",
    "            print(f\"Predicted sentiment: {sentiment} (confidence: {confidence:.4f})\")\n",
    "            print()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Model not found. Please run training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4b0f0-6e6a-4817-8178-b9a26cffe264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
