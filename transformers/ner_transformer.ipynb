{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "fbe58c05-6edf-4b3f-bd1d-54e4fee64a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "715912ff-e093-4a76-9970-c5b1efb71864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self):\n",
    "        self.names = ['john', 'jane', 'bob', 'alice', 'charlie', 'diana', 'jamy12354']\n",
    "        self.domains = ['gmail.com', 'yahoo.com', 'outlook.com', 'company.com']\n",
    "        \n",
    "    def generate_phone(self):\n",
    "        area = f\"{random.randint(100,999)}\"\n",
    "        exchange = f\"{random.randint(100,999)}\"\n",
    "        number = f\"{random.randint(1000,9999)}\"\n",
    "        \n",
    "        formats = [\n",
    "            f\"({area}) {exchange}-{number}\",\n",
    "            f\"{area}-{exchange}-{number}\",\n",
    "            f\"{area}.{exchange}.{number}\",\n",
    "            f\"{area} {exchange} {number}\"\n",
    "        ]\n",
    "        return random.choice(formats)\n",
    "    \n",
    "    def generate_email(self):\n",
    "        name = random.choice(self.names)\n",
    "        num = random.randint(1, 99)\n",
    "        domain = random.choice(self.domains)\n",
    "        return f\"{name}{num}@{domain}\"\n",
    "    \n",
    "    def generate_sample(self):\n",
    "        phone_templates = [\n",
    "            \"call me at {entity}\",\n",
    "            \"my number is {entity}\",\n",
    "            \"contact me at my {entity} phone for info\",\n",
    "            \"reach out to {entity}\",\n",
    "            \"you can call {entity}\",\n",
    "            \"{entity} is my contact\",\n",
    "            \"here is my contact info: {entity}\",            \n",
    "        ]\n",
    "\n",
    "        email_templates = [\n",
    "            \"contact my by sending an email at {entity} for info\",\n",
    "            \"email me at {entity} email address\",\n",
    "            \"email {entity} address\",\n",
    "            \"reach out to {entity}\",\n",
    "            \"send email to {entity}\",\n",
    "            \"{entity} is my contact\",\n",
    "            \"here is my contact info: {entity}\",            \n",
    "        ]\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            entity = self.generate_phone()\n",
    "            entity_type = \"PHONE\"\n",
    "            templates = phone_templates\n",
    "        else:\n",
    "            entity = self.generate_email()\n",
    "            entity_type = \"EMAIL\"\n",
    "            templates = email_templates\n",
    "            \n",
    "        template = random.choice(templates)\n",
    "        text = template.format(entity=entity)\n",
    "        \n",
    "        return text, entity, entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "89d376ce-dc99-4048-925e-7080e70735b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, max_length=64):\n",
    "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<NUMBER>': 2, '<LETTER>': 3, '<SPECIAL>': 4}\n",
    "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<NUMBER>', 3: '<LETTER>', 4: '<SPECIAL>'}\n",
    "        self.vocab_size = 5\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Pre-add common characters and numbers to avoid UNK\n",
    "        all_chars = list('abcdefghijklmnopqrstuvwxyz0123456789.-@():/+')\n",
    "        \n",
    "        self.special_chars = list('.-@():/+')\n",
    "        self.letter_chars = list('abcdefghijklmnopqrstuvwxyz')\n",
    "        self.number_chars = list('0123456789')\n",
    "        \n",
    "        for char in all_chars:            \n",
    "            if char not in self.word_to_idx:\n",
    "                self.word_to_idx[char] = self.vocab_size\n",
    "                self.idx_to_word[self.vocab_size] = char\n",
    "                self.vocab_size += 1\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        words = set()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            words.update(tokens)\n",
    "        \n",
    "        # Add all unique tokens to vocabulary\n",
    "        for word in sorted(words):\n",
    "            if word not in self.word_to_idx:\n",
    "                self.word_to_idx[word] = self.vocab_size\n",
    "                self.idx_to_word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "                \n",
    "        print(f\"Built vocabulary with {self.vocab_size} tokens\")\n",
    "        print(f\"Sample vocab: {list(self.word_to_idx.keys())}\")\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        # Character-level tokenization for better handling of numbers and punctuation\n",
    "        text = text.lower().strip()\n",
    "        tokens = []\n",
    "        \n",
    "        # Split into words first, then handle each word\n",
    "        words = text.split()\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0:  # Add space token between words\n",
    "                tokens.append(' ')\n",
    "            \n",
    "            # For each word, split into meaningful chunks\n",
    "            current_token = \"\"\n",
    "            for char in word:\n",
    "                if char in self.number_chars:\n",
    "                    current_token += '<NUMBER>'\n",
    "                if char in self.letter_chars:\n",
    "                    current_token += '<LETTER>'\n",
    "                if char in self.special_chars:\n",
    "                    current_token += char\n",
    "                else:\n",
    "                    # Punctuation - add current token and the punctuation\n",
    "                    if current_token:\n",
    "                        tokens.append(current_token)\n",
    "                        current_token = \"\"\n",
    "                    tokens.append(char)\n",
    "            \n",
    "            # Add any remaining token\n",
    "            if current_token:\n",
    "                tokens.append(current_token)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        indices = [self.word_to_idx.get(token, 1) for token in tokens]\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(indices) < self.max_length:\n",
    "            indices.extend([0] * (self.max_length - len(indices)))\n",
    "        else:\n",
    "            indices = indices[:self.max_length]\n",
    "            \n",
    "        return indices, len([idx for idx in indices if idx != 0])  # Return actual length\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return [self.idx_to_word.get(idx, '<UNK>') for idx in indices if idx != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "8d59f3ce-898f-44c1-850d-df886ae9a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_for_text(text, entity, entity_type, tokenizer):\n",
    "    \"\"\"Create BIO labels for a text given the entity and its type\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    entity_tokens = tokenizer.tokenize(entity)\n",
    "    \n",
    "    labels = [0] * len(tokens)  # Start with all O (outside)\n",
    "    \n",
    "    # More robust entity matching - look for subsequences\n",
    "    def find_entity_in_tokens(tokens, entity_tokens):\n",
    "        for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "            # Check if entity tokens match (ignoring spaces)\n",
    "            entity_idx = 0\n",
    "            match_positions = []\n",
    "            \n",
    "            for j in range(i, len(tokens)):\n",
    "                if entity_idx >= len(entity_tokens):\n",
    "                    break\n",
    "                    \n",
    "                if tokens[j] == entity_tokens[entity_idx]:\n",
    "                    match_positions.append(j)\n",
    "                    entity_idx += 1\n",
    "                elif tokens[j] == ' ':\n",
    "                    continue  # Skip spaces\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if entity_idx == len(entity_tokens):\n",
    "                return match_positions\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    # Find entity positions\n",
    "    entity_positions = find_entity_in_tokens(tokens, entity_tokens)\n",
    "    \n",
    "    if entity_positions:\n",
    "        # Mark the entity tokens\n",
    "        if entity_type == \"PHONE\":\n",
    "            labels[entity_positions[0]] = 1  # B-PHONE\n",
    "            for pos in entity_positions[1:]:\n",
    "                labels[pos] = 2  # I-PHONE\n",
    "        else:  # EMAIL\n",
    "            labels[entity_positions[0]] = 3  # B-EMAIL\n",
    "            for pos in entity_positions[1:]:\n",
    "                labels[pos] = 4  # I-EMAIL\n",
    "    \n",
    "    # Pad labels to match tokenizer max_length\n",
    "    while len(labels) < tokenizer.max_length:\n",
    "        labels.append(0)\n",
    "    labels = labels[:tokenizer.max_length]\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "fad02453-5888-4395-a612-8f400114a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(model, tokenizer, text):\n",
    "    \"\"\"Extract phone numbers and emails from text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    tokens, actual_length = tokenizer.encode(text)\n",
    "    tokens_tensor = torch.tensor(tokens).unsqueeze(0)\n",
    "    attention_mask = torch.tensor([1] * actual_length + [0] * (len(tokens) - actual_length)).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, attention_mask)\n",
    "        predictions = torch.argmax(outputs, dim=-1).squeeze().tolist()\n",
    "\n",
    "    # Decode predictions\n",
    "    label_names = ['O', 'B-PHONE', 'I-PHONE', 'B-EMAIL', 'I-EMAIL']\n",
    "    predicted_labels = [label_names[pred] for pred in predictions[:actual_length]]\n",
    "    \n",
    "    # Get actual tokens\n",
    "    actual_tokens = tokenizer.decode(tokens)[:actual_length]\n",
    "    \n",
    "    print(f\"Actual - Tokens: {actual_tokens}\")\n",
    "    print(f\"Predicted - Labels: {predicted_labels}\")\n",
    "    \n",
    "    # Extract entities with better reconstruction\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_type = None\n",
    "    \n",
    "    for token, label in zip(actual_tokens, predicted_labels):\n",
    "        if label.startswith('B-'):\n",
    "            # Save previous entity if exists\n",
    "            if current_entity:\n",
    "                entity_text = ''.join(current_entity).replace(' ', '')\n",
    "                entities.append((current_type, entity_text))\n",
    "            \n",
    "            # Start new entity\n",
    "            current_entity = [token] if token != ' ' else []\n",
    "            current_type = label[2:]  # Remove 'B-'\n",
    "            \n",
    "        elif label.startswith('I-') and current_type and current_type == label[2:]:\n",
    "            if token != ' ':  # Don't add spaces to entity\n",
    "                current_entity.append(token)\n",
    "        else:\n",
    "            # End current entity\n",
    "            if current_entity:\n",
    "                entity_text = ''.join(current_entity).replace(' ', '')\n",
    "                entities.append((current_type, entity_text))\n",
    "            current_entity = []\n",
    "            current_type = None\n",
    "    \n",
    "    # Don't forget the last entity\n",
    "    if current_entity:\n",
    "        entity_text = ''.join(current_entity).replace(' ', '')\n",
    "        entities.append((current_type, entity_text))\n",
    "    \n",
    "    return entities, list(zip(actual_tokens, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "61e2b676-6c95-4ed6-8ce0-13bcbafe2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerExtractor(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=2, num_classes=5, max_seq_len=64):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=256,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Add a final layer norm and dropout\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, 0, 0.1)\n",
    "                if module.padding_idx is not None:\n",
    "                    module.weight.data[module.padding_idx].zero_()\n",
    "    \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Create padding mask for transformer\n",
    "        if attention_mask is None:\n",
    "            padding_mask = (x == 0)  # True for padding tokens\n",
    "        else:\n",
    "            padding_mask = ~attention_mask.bool()\n",
    "        \n",
    "        # Embedding with proper scaling\n",
    "        x = self.embedding(x) * (self.d_model ** 0.5)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Apply transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Final processing\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class ExtractionDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, entity, entity_type = self.samples[idx]\n",
    "        \n",
    "        tokens, actual_length = self.tokenizer.encode(text)\n",
    "        labels = create_labels_for_text(text, entity, entity_type, self.tokenizer)\n",
    "        \n",
    "        return {\n",
    "            'tokens': torch.tensor(tokens, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1] * actual_length + [0] * (len(tokens) - actual_length), dtype=torch.bool)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tokens = torch.stack([item['tokens'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    return tokens, labels, attention_mask\n",
    "\n",
    "def train_model(num_samples=2000, epochs=15):\n",
    "    print(\"Generating training data...\")\n",
    "    \n",
    "    # Generate data\n",
    "    generator = DataGenerator()\n",
    "    samples = [generator.generate_sample() for _ in range(num_samples)]\n",
    "    \n",
    "    # Build tokenizer\n",
    "    tokenizer = SimpleTokenizer(max_length=64)\n",
    "    texts = [sample[0] for sample in samples]\n",
    "    tokenizer.build_vocab(texts)\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(0.8 * len(samples))\n",
    "    train_samples = samples[:split_idx]\n",
    "    val_samples = samples[split_idx:]\n",
    "    \n",
    "    print(f\"Training samples: {len(train_samples)}, Validation samples: {len(val_samples)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = ExtractionDataset(train_samples, tokenizer)\n",
    "    val_dataset = ExtractionDataset(val_samples, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SimpleTransformerExtractor(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        d_model=128,\n",
    "        nhead=8,\n",
    "        num_layers=2,\n",
    "        num_classes=5,\n",
    "        max_seq_len=64\n",
    "    )\n",
    "    \n",
    "    # Use class weights to handle imbalanced data\n",
    "    class_weights = torch.tensor([0.1, 2.0, 2.0, 2.0, 2.0])  # Lower weight for O, higher for entities\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "    \n",
    "    label_names = ['O', 'B-PHONE', 'I-PHONE', 'B-EMAIL', 'I-EMAIL']\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch_idx, (tokens, labels, attention_mask) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(tokens, attention_mask)\n",
    "            \n",
    "            # Only compute loss on non-padded tokens\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = outputs.view(-1, 5)[active_loss]\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "            \n",
    "            loss = criterion(active_logits, active_labels)\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                print(f\"NaN loss detected at batch {batch_idx}\")\n",
    "                continue\n",
    "                \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(active_logits, dim=-1)\n",
    "            correct_predictions += (predictions == active_labels).sum().item()\n",
    "            total_predictions += active_labels.size(0)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tokens, labels, attention_mask in val_loader:\n",
    "                outputs = model(tokens, attention_mask)\n",
    "                \n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = outputs.view(-1, 5)[active_loss]\n",
    "                active_labels = labels.view(-1)[active_loss]\n",
    "                \n",
    "                loss = criterion(active_logits, active_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(active_logits, dim=-1)\n",
    "                val_correct += (predictions == active_labels).sum().item()\n",
    "                val_total += active_labels.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"  New best validation loss!\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "8a130a21-aad8-42ac-9026-671d68457c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training transformer model for phone/email extraction...\n",
      "Generating training data...\n",
      "Built vocabulary with 55 tokens\n",
      "Sample vocab: ['<PAD>', '<UNK>', '<NUMBER>', '<LETTER>', '<SPECIAL>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.', '-', '@', '(', ')', ':', '/', '+', ' ', '(<NUMBER>', '-<NUMBER>', '.<LETTER>', '.<NUMBER>', '@<LETTER>']\n",
      "Training samples: 2400, Validation samples: 600\n",
      "Starting training...\n",
      "Epoch 1/5\n",
      "  Train Loss: 0.2534, Train Acc: 0.7292\n",
      "  Val Loss: 0.0692, Val Acc: 0.9036\n",
      "  New best validation loss!\n",
      "Epoch 2/5\n",
      "  Train Loss: 0.0503, Train Acc: 0.9160\n",
      "  Val Loss: 0.0172, Val Acc: 0.9422\n",
      "  New best validation loss!\n",
      "Epoch 3/5\n",
      "  Train Loss: 0.0153, Train Acc: 0.9738\n",
      "  Val Loss: 0.0023, Val Acc: 0.9966\n",
      "  New best validation loss!\n",
      "Epoch 4/5\n",
      "  Train Loss: 0.0073, Train Acc: 0.9893\n",
      "  Val Loss: 0.0033, Val Acc: 0.9983\n",
      "Epoch 5/5\n",
      "  Train Loss: 0.0037, Train Acc: 0.9933\n",
      "  Val Loss: 0.0013, Val Acc: 0.9969\n",
      "  New best validation loss!\n"
     ]
    }
   ],
   "source": [
    "# Train the transformer\n",
    "print(\"Training transformer model for phone/email extraction...\")\n",
    "\n",
    "# Train the model\n",
    "model, tokenizer = train_model(num_samples=3000, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "c384642c-ce77-412a-8d1e-1b32b38fb64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing extraction:\n",
      "==================================================\n",
      "\n",
      "Original text: call me at (555) 123-4567\n",
      "Tokenized: ['<LETTER>', 'c', '<LETTER>', 'a', '<LETTER>', 'l', '<LETTER>', 'l', ' ', '<LETTER>', 'm', '<LETTER>', 'e', ' ', '<LETTER>', 'a', '<LETTER>', 't', ' ', '(<NUMBER>', '5', '<NUMBER>', '5', '<NUMBER>', '5', ')', ' ', '<NUMBER>', '1', '<NUMBER>', '2', '<NUMBER>', '3', '-<NUMBER>', '4', '<NUMBER>', '5', '<NUMBER>', '6', '<NUMBER>', '7']\n",
      "Actual - Tokens: ['<LETTER>', 'c', '<LETTER>', 'a', '<LETTER>', 'l', '<LETTER>', 'l', ' ', '<LETTER>', 'm', '<LETTER>', 'e', ' ', '<LETTER>', 'a', '<LETTER>', 't', ' ', '(<NUMBER>', '5', '<NUMBER>', '5', '<NUMBER>', '5', ')', ' ', '<NUMBER>', '1', '<NUMBER>', '2', '<NUMBER>', '3', '-<NUMBER>', '4', '<NUMBER>', '5', '<NUMBER>', '6', '<NUMBER>', '7']\n",
      "Predicted - Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE']\n",
      "Extracted entities: {'PHONE': '(555)123-4567'}\n",
      "------------------------------\n",
      "\n",
      "Original text: email me at john@gmail.com\n",
      "Tokenized: ['<LETTER>', 'e', '<LETTER>', 'm', '<LETTER>', 'a', '<LETTER>', 'i', '<LETTER>', 'l', ' ', '<LETTER>', 'm', '<LETTER>', 'e', ' ', '<LETTER>', 'a', '<LETTER>', 't', ' ', '<LETTER>', 'j', '<LETTER>', 'o', '<LETTER>', 'h', '<LETTER>', 'n', '@<LETTER>', 'g', '<LETTER>', 'm', '<LETTER>', 'a', '<LETTER>', 'i', '<LETTER>', 'l', '.<LETTER>', 'c', '<LETTER>', 'o', '<LETTER>', 'm']\n",
      "Actual - Tokens: ['<LETTER>', 'e', '<LETTER>', 'm', '<LETTER>', 'a', '<LETTER>', 'i', '<LETTER>', 'l', ' ', '<LETTER>', 'm', '<LETTER>', 'e', ' ', '<LETTER>', 'a', '<LETTER>', 't', ' ', '<LETTER>', 'j', '<LETTER>', 'o', '<LETTER>', 'h', '<LETTER>', 'n', '@<LETTER>', 'g', '<LETTER>', 'm', '<LETTER>', 'a', '<LETTER>', 'i', '<LETTER>', 'l', '.<LETTER>', 'c', '<LETTER>', 'o', '<LETTER>', 'm']\n",
      "Predicted - Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL']\n",
      "Extracted entities: {'EMAIL': 'john@gmail.com'}\n",
      "------------------------------\n",
      "\n",
      "Original text: here is my cell (343) 232 554\n",
      "Tokenized: ['<LETTER>', 'h', '<LETTER>', 'e', '<LETTER>', 'r', '<LETTER>', 'e', ' ', '<LETTER>', 'i', '<LETTER>', 's', ' ', '<LETTER>', 'm', '<LETTER>', 'y', ' ', '<LETTER>', 'c', '<LETTER>', 'e', '<LETTER>', 'l', '<LETTER>', 'l', ' ', '(<NUMBER>', '3', '<NUMBER>', '4', '<NUMBER>', '3', ')', ' ', '<NUMBER>', '2', '<NUMBER>', '3', '<NUMBER>', '2', ' ', '<NUMBER>', '5', '<NUMBER>', '5', '<NUMBER>', '4']\n",
      "Actual - Tokens: ['<LETTER>', 'h', '<LETTER>', 'e', '<LETTER>', 'r', '<LETTER>', 'e', ' ', '<LETTER>', 'i', '<LETTER>', 's', ' ', '<LETTER>', 'm', '<LETTER>', 'y', ' ', '<LETTER>', 'c', '<LETTER>', 'e', '<LETTER>', 'l', '<LETTER>', 'l', ' ', '(<NUMBER>', '3', '<NUMBER>', '4', '<NUMBER>', '3', ')', ' ', '<NUMBER>', '2', '<NUMBER>', '3', '<NUMBER>', '2', ' ', '<NUMBER>', '5', '<NUMBER>', '5', '<NUMBER>', '4']\n",
      "Predicted - Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PHONE', 'O', 'B-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE']\n",
      "Extracted entities: {'PHONE': '(343)232554'}\n",
      "------------------------------\n",
      "\n",
      "Original text: email me at j123@gmail.com\n",
      "Tokenized: ['<LETTER>', 'e', '<LETTER>', 'm', '<LETTER>', 'a', '<LETTER>', 'i', '<LETTER>', 'l', ' ', '<LETTER>', 'm', '<LETTER>', 'e', ' ', '<LETTER>', 'a', '<LETTER>', 't', ' ', '<LETTER>', 'j', '<NUMBER>', '1', '<NUMBER>', '2', '<NUMBER>', '3', '@<LETTER>', 'g', '<LETTER>', 'm', '<LETTER>', 'a', '<LETTER>', 'i', '<LETTER>', 'l', '.<LETTER>', 'c', '<LETTER>', 'o', '<LETTER>', 'm']\n",
      "Actual - Tokens: ['<LETTER>', 'e', '<LETTER>', 'm', '<LETTER>', 'a', '<LETTER>', 'i', '<LETTER>', 'l', ' ', '<LETTER>', 'm', '<LETTER>', 'e', ' ', '<LETTER>', 'a', '<LETTER>', 't', ' ', '<LETTER>', 'j', '<NUMBER>', '1', '<NUMBER>', '2', '<NUMBER>', '3', '@<LETTER>', 'g', '<LETTER>', 'm', '<LETTER>', 'a', '<LETTER>', 'i', '<LETTER>', 'l', '.<LETTER>', 'c', '<LETTER>', 'o', '<LETTER>', 'm']\n",
      "Predicted - Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL']\n",
      "Extracted entities: {'EMAIL': 'j123@gmail.com'}\n",
      "------------------------------\n",
      "\n",
      "Original text: cell number 222 343-234\n",
      "Tokenized: ['<LETTER>', 'c', '<LETTER>', 'e', '<LETTER>', 'l', '<LETTER>', 'l', ' ', '<LETTER>', 'n', '<LETTER>', 'u', '<LETTER>', 'm', '<LETTER>', 'b', '<LETTER>', 'e', '<LETTER>', 'r', ' ', '<NUMBER>', '2', '<NUMBER>', '2', '<NUMBER>', '2', ' ', '<NUMBER>', '3', '<NUMBER>', '4', '<NUMBER>', '3', '-<NUMBER>', '2', '<NUMBER>', '3', '<NUMBER>', '4']\n",
      "Actual - Tokens: ['<LETTER>', 'c', '<LETTER>', 'e', '<LETTER>', 'l', '<LETTER>', 'l', ' ', '<LETTER>', 'n', '<LETTER>', 'u', '<LETTER>', 'm', '<LETTER>', 'b', '<LETTER>', 'e', '<LETTER>', 'r', ' ', '<NUMBER>', '2', '<NUMBER>', '2', '<NUMBER>', '2', ' ', '<NUMBER>', '3', '<NUMBER>', '4', '<NUMBER>', '3', '-<NUMBER>', '2', '<NUMBER>', '3', '<NUMBER>', '4']\n",
      "Predicted - Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE']\n",
      "Extracted entities: {'PHONE': '222343-234'}\n",
      "------------------------------\n",
      "\n",
      "Original text: contact info: (121) 334-5456\n",
      "Tokenized: ['<LETTER>', 'c', '<LETTER>', 'o', '<LETTER>', 'n', '<LETTER>', 't', '<LETTER>', 'a', '<LETTER>', 'c', '<LETTER>', 't', ' ', '<LETTER>', 'i', '<LETTER>', 'n', '<LETTER>', 'f', '<LETTER>', 'o', ':', ' ', '(<NUMBER>', '1', '<NUMBER>', '2', '<NUMBER>', '1', ')', ' ', '<NUMBER>', '3', '<NUMBER>', '3', '<NUMBER>', '4', '-<NUMBER>', '5', '<NUMBER>', '4', '<NUMBER>', '5', '<NUMBER>', '6']\n",
      "Actual - Tokens: ['<LETTER>', 'c', '<LETTER>', 'o', '<LETTER>', 'n', '<LETTER>', 't', '<LETTER>', 'a', '<LETTER>', 'c', '<LETTER>', 't', ' ', '<LETTER>', 'i', '<LETTER>', 'n', '<LETTER>', 'f', '<LETTER>', 'o', ':', ' ', '(<NUMBER>', '1', '<NUMBER>', '2', '<NUMBER>', '1', ')', ' ', '<NUMBER>', '3', '<NUMBER>', '3', '<NUMBER>', '4', '-<NUMBER>', '5', '<NUMBER>', '4', '<NUMBER>', '5', '<NUMBER>', '6']\n",
      "Predicted - Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE', 'I-PHONE']\n",
      "Extracted entities: {'PHONE': '(121)334-5456'}\n",
      "------------------------------\n",
      "\n",
      "Original text: contact info: mike@text.ai\n",
      "Tokenized: ['<LETTER>', 'c', '<LETTER>', 'o', '<LETTER>', 'n', '<LETTER>', 't', '<LETTER>', 'a', '<LETTER>', 'c', '<LETTER>', 't', ' ', '<LETTER>', 'i', '<LETTER>', 'n', '<LETTER>', 'f', '<LETTER>', 'o', ':', ' ', '<LETTER>', 'm', '<LETTER>', 'i', '<LETTER>', 'k', '<LETTER>', 'e', '@<LETTER>', 't', '<LETTER>', 'e', '<LETTER>', 'x', '<LETTER>', 't', '.<LETTER>', 'a', '<LETTER>', 'i']\n",
      "Actual - Tokens: ['<LETTER>', 'c', '<LETTER>', 'o', '<LETTER>', 'n', '<LETTER>', 't', '<LETTER>', 'a', '<LETTER>', 'c', '<LETTER>', 't', ' ', '<LETTER>', 'i', '<LETTER>', 'n', '<LETTER>', 'f', '<LETTER>', 'o', ':', ' ', '<LETTER>', 'm', '<LETTER>', 'i', '<LETTER>', 'k', '<LETTER>', 'e', '@<LETTER>', 't', '<LETTER>', 'e', '<LETTER>', 'x', '<LETTER>', 't', '.<LETTER>', 'a', '<LETTER>', 'i']\n",
      "Predicted - Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL']\n",
      "Extracted entities: {'EMAIL': 'mike@text.ai'}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test on examples\n",
    "test_texts = [\n",
    "    \"call me at (555) 123-4567\",\n",
    "    \"email me at john@gmail.com\",\n",
    "    \"here is my cell (343) 232 554\",\n",
    "    \"email me at j123@gmail.com\",    \n",
    "    \"cell number 222 343-234\",\n",
    "    \"contact info: (121) 334-5456\",\n",
    "    \"contact info: mike@text.ai\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing extraction:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\nOriginal text: {text}\")\n",
    "    \n",
    "    # Show tokenization\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"Tokenized: {tokens}\")\n",
    "    \n",
    "    # Check vocabulary coverage\n",
    "    unknown_tokens = [t for t in tokens if tokenizer.word_to_idx.get(t) == 1]\n",
    "    if unknown_tokens:\n",
    "        print(f\"Unknown tokens: {unknown_tokens}\")\n",
    "    \n",
    "    entities, token_labels = extract_entities(model, tokenizer, text)\n",
    "    extracted = dict()\n",
    "    for entity in entities:\n",
    "        name, value = entity\n",
    "        value = str(value)\n",
    "        for symbol in ['<LETTER>', '<NUMBER>']:\n",
    "            value = value.replace(symbol, '')            \n",
    "        extracted[name] = value\n",
    "    print(f\"Extracted entities: {extracted}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
