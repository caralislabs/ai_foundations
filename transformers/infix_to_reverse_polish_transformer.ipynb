{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7e780-3f4c-45c9-b9cb-46f6b1a32ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3aca59fe-1555-40a9-a634-1be6ce8e57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "339d7bc2-e5aa-4625-8eea-f2ffa230a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenVocabulary:\n",
    "    def __init__(self):\n",
    "        # Special tokens first (PAD should be 0)\n",
    "        self.special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "        \n",
    "        # Mathematical operators and symbols\n",
    "        self.operators = ['+', '-', '*', '/', '^', '(', ')']\n",
    "        self.digits = []\n",
    "        self.variables = ['a', 'b', 'c']\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.vocab = self.special_tokens + self.operators + self.digits + self.variables\n",
    "        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}\n",
    "        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
    "        \n",
    "        self.pad_id = self.token_to_id['<PAD>']\n",
    "        self.sos_id = self.token_to_id['<SOS>']\n",
    "        self.eos_id = self.token_to_id['<EOS>']\n",
    "        self.unk_id = self.token_to_id['<UNK>']\n",
    "        print(f'vocab: {self.vocab}')\n",
    "    \n",
    "    def tokenize(self, expression: str) -> List[str]:\n",
    "        \"\"\"Tokenize mathematical expression\"\"\"\n",
    "        expression = expression.replace(' ', '')\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(expression):\n",
    "            char = expression[i]\n",
    "            if char.isdigit():\n",
    "                # Handle multi-digit numbers\n",
    "                num = ''\n",
    "                while i < len(expression) and expression[i].isdigit():\n",
    "                    num += expression[i]\n",
    "                    i += 1\n",
    "                tokens.append(num)\n",
    "            elif char in self.operators or char.isalpha():\n",
    "                tokens.append(char)\n",
    "                i += 1\n",
    "            else:\n",
    "                i += 1  # Skip unknown characters\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        \"\"\"Convert tokens to IDs\"\"\"\n",
    "        return [self.token_to_id.get(token, self.unk_id) for token in tokens]\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        \"\"\"Convert IDs to tokens\"\"\"\n",
    "        return [self.id_to_token[id] for id in ids if id != self.pad_id]\n",
    "\n",
    "class InfixToPolishConverter:\n",
    "    \"\"\"Convert infix notation to Polish notation using Shunting Yard algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.precedence = {'+': 1, '-': 1, '*': 2, '/': 2, '^': 3}\n",
    "        self.right_associative = {'^'}\n",
    "    \n",
    "    def infix_to_polish(self, infix_tokens: List[str]) -> List[str]:\n",
    "        \"\"\"Convert infix tokens to Polish notation\"\"\"\n",
    "        output = []\n",
    "        operator_stack = []\n",
    "        \n",
    "        # Reverse input and flip parentheses for Polish notation\n",
    "        reversed_tokens = []\n",
    "        for token in reversed(infix_tokens):\n",
    "            if token == '(':\n",
    "                reversed_tokens.append(')')\n",
    "            elif token == ')':\n",
    "                reversed_tokens.append('(')\n",
    "            else:\n",
    "                reversed_tokens.append(token)\n",
    "        \n",
    "        # Modified Shunting Yard for prefix notation\n",
    "        for token in reversed_tokens:\n",
    "            if token.isdigit() or token.isalpha():\n",
    "                output.append(token)\n",
    "            elif token == '(':\n",
    "                operator_stack.append(token)\n",
    "            elif token == ')':\n",
    "                while operator_stack and operator_stack[-1] != '(':\n",
    "                    output.append(operator_stack.pop())\n",
    "                if operator_stack:\n",
    "                    operator_stack.pop()\n",
    "            elif token in self.precedence:\n",
    "                while (operator_stack and \n",
    "                       operator_stack[-1] != '(' and\n",
    "                       operator_stack[-1] in self.precedence and\n",
    "                       (self.precedence[operator_stack[-1]] > self.precedence[token] or\n",
    "                        (self.precedence[operator_stack[-1]] == self.precedence[token] and \n",
    "                         token not in self.right_associative))):\n",
    "                    output.append(operator_stack.pop())\n",
    "                operator_stack.append(token)\n",
    "        \n",
    "        while operator_stack:\n",
    "            output.append(operator_stack.pop())\n",
    "        \n",
    "        return list(reversed(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "393fd69b-ae04-48be-b244-8b3cf1957566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class InfixPolishTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int = 128, nhead: int = 4, \n",
    "                 num_encoder_layers: int = 3, num_decoder_layers: int = 3,\n",
    "                 dim_feedforward: int = 256, max_len: int = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Embeddings\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_projection.bias.data.zero_()\n",
    "        self.output_projection.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, \n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        # Embeddings\n",
    "        src_emb = self.pos_encoding(self.encoder_embedding(src) * math.sqrt(self.d_model))\n",
    "        tgt_emb = self.pos_encoding(self.decoder_embedding(tgt) * math.sqrt(self.d_model))\n",
    "        \n",
    "        # Transformer forward pass\n",
    "        output = self.transformer(\n",
    "            src_emb, tgt_emb,\n",
    "            src_mask=src_mask, tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        return self.output_projection(output)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "61692b4e-c89f-486d-914b-b312a578b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfixPolishDataset:\n",
    "    def __init__(self, vocab: TokenVocabulary, converter: InfixToPolishConverter):\n",
    "        self.vocab = vocab\n",
    "        self.converter = converter\n",
    "    \n",
    "    def generate_simple_expression(self) -> str:\n",
    "        \"\"\"Generate simple infix expressions\"\"\"\n",
    "        operators = ['+', '-', '*', '/']\n",
    "        # operands = ['a', 'b', 'c'] + ['1', '2', '3', '4', '5']\n",
    "        operands = ['a', 'b', 'c']\n",
    "        \n",
    "        # Generate different types of expressions\n",
    "        expr_type = random.choice(['binary', 'chain', 'paren'])\n",
    "        \n",
    "        if expr_type == 'binary':\n",
    "            # Simple: a + b\n",
    "            op1 = random.choice(operands)\n",
    "            operator = random.choice(operators)\n",
    "            op2 = random.choice(operands)\n",
    "            return f\"{op1}{operator}{op2}\"\n",
    "        \n",
    "        elif expr_type == 'chain':\n",
    "            # Chain: a + b * c\n",
    "            op1 = random.choice(operands)\n",
    "            op1_op = random.choice(operators)\n",
    "            op2 = random.choice(operands)\n",
    "            op2_op = random.choice(operators)\n",
    "            op3 = random.choice(operands)\n",
    "            return f\"{op1}{op1_op}{op2}{op2_op}{op3}\"\n",
    "        \n",
    "        else:  # paren\n",
    "            # Parentheses: (a + b) * c\n",
    "            op1 = random.choice(operands)\n",
    "            op1_op = random.choice(operators)\n",
    "            op2 = random.choice(operands)\n",
    "            op2_op = random.choice(operators)\n",
    "            op3 = random.choice(operands)\n",
    "            return f\"({op1}{op1_op}{op2}){op2_op}{op3}\"\n",
    "    \n",
    "    def create_training_pair(self, infix_expr: str) -> Tuple[List[int], List[int]]:\n",
    "        \"\"\"Create training pair from infix expression\"\"\"\n",
    "        infix_tokens = self.vocab.tokenize(infix_expr)\n",
    "        polish_tokens = self.converter.infix_to_polish(infix_tokens)\n",
    "        \n",
    "        infix_ids = self.vocab.encode(infix_tokens)\n",
    "        polish_ids = [self.vocab.sos_id] + self.vocab.encode(polish_tokens) + [self.vocab.eos_id]\n",
    "        \n",
    "        return infix_ids, polish_ids\n",
    "\n",
    "def create_padding_mask(sequences, pad_id):\n",
    "    \"\"\"Create padding mask for sequences\"\"\"\n",
    "    return (sequences == pad_id)\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    \"\"\"Collate function for DataLoader\"\"\"\n",
    "    src_sequences, tgt_sequences = zip(*batch)\n",
    "    \n",
    "    max_src_len = max(len(seq) for seq in src_sequences)\n",
    "    max_tgt_len = max(len(seq) for seq in tgt_sequences)\n",
    "    \n",
    "    padded_src = []\n",
    "    padded_tgt = []\n",
    "    \n",
    "    for src, tgt in zip(src_sequences, tgt_sequences):\n",
    "        padded_src.append(src + [pad_id] * (max_src_len - len(src)))\n",
    "        padded_tgt.append(tgt + [pad_id] * (max_tgt_len - len(tgt)))\n",
    "    \n",
    "    return torch.tensor(padded_src), torch.tensor(padded_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "98f4b1f1-bb4e-4639-9fa2-ed75c3d30dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    print(\"Initializing components...\")\n",
    "    vocab = TokenVocabulary()\n",
    "    converter = InfixToPolishConverter()\n",
    "    dataset = InfixPolishDataset(vocab, converter)\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocab.vocab)}\")\n",
    "    print(f\"Sample vocab: {vocab.vocab[:15]}\")\n",
    "    \n",
    "    # Test converter first\n",
    "    test_expr = \"b+c*a\"\n",
    "    test_tokens = vocab.tokenize(test_expr)\n",
    "    test_polish = converter.infix_to_polish(test_tokens)\n",
    "    print(f\"Test conversion: {test_expr} -> {test_tokens} -> {test_polish}\")\n",
    "    \n",
    "    model = InfixPolishTransformer(vocab_size=len(vocab.vocab))\n",
    "    \n",
    "    # Generate training data\n",
    "    print(\"Generating training data...\")\n",
    "    training_pairs = []\n",
    "    \n",
    "    for i in range(3000):\n",
    "        try:\n",
    "            expr = dataset.generate_simple_expression()\n",
    "            pair = dataset.create_training_pair(expr)\n",
    "            \n",
    "            # Validate the pair\n",
    "            if len(pair[0]) > 0 and len(pair[1]) > 2:  # Must have content\n",
    "                training_pairs.append(pair)\n",
    "                \n",
    "                if i < 5:  # Print first few examples\n",
    "                    infix_tokens = vocab.tokenize(expr)\n",
    "                    polish_tokens = converter.infix_to_polish(infix_tokens)\n",
    "                    print(f\"Example {i+1}: {expr}\")\n",
    "                    print(f\"  Infix: {infix_tokens} -> {pair[0]}\")\n",
    "                    print(f\"  Polish: {polish_tokens} -> {pair[1]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if i < 10:\n",
    "                print(f\"Error with expression: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Generated {len(training_pairs)} valid training pairs\")\n",
    "    \n",
    "    if len(training_pairs) < 100:\n",
    "        print(\"Warning: Very few training pairs generated!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.pad_id)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(25):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        random.shuffle(training_pairs)\n",
    "        batch_size = 16\n",
    "        \n",
    "        for i in range(0, len(training_pairs), batch_size):\n",
    "            batch = training_pairs[i:i+batch_size]\n",
    "            if len(batch) < 2:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                src_batch, tgt_batch = collate_fn(batch, vocab.pad_id)\n",
    "                \n",
    "                tgt_input = tgt_batch[:, :-1]\n",
    "                tgt_output = tgt_batch[:, 1:]\n",
    "                \n",
    "                if tgt_input.size(1) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Create masks\n",
    "                src_key_padding_mask = create_padding_mask(src_batch, vocab.pad_id)\n",
    "                tgt_key_padding_mask = create_padding_mask(tgt_input, vocab.pad_id)\n",
    "                tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                output = model(src_batch, tgt_input, \n",
    "                             tgt_mask=tgt_mask,\n",
    "                             src_key_padding_mask=src_key_padding_mask,\n",
    "                             tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "                \n",
    "                loss = criterion(output.reshape(-1, len(vocab.vocab)), tgt_output.reshape(-1))\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}, Avg Loss: {avg_loss:.4f}, Batches: {num_batches}\")\n",
    "        \n",
    "        # Test every 5 epochs\n",
    "        if epoch % 5 == 0 and epoch > 0:\n",
    "            model.eval()\n",
    "            test_expr = \"a+b\"\n",
    "            predicted = convert_infix_to_polish(model, vocab, converter, test_expr)\n",
    "            actual = converter.infix_to_polish(vocab.tokenize(test_expr))\n",
    "            print(f\"  Test: {test_expr} -> Pred: '{predicted}' | Actual: '{' '.join(actual)}'\")\n",
    "            model.train()\n",
    "    \n",
    "    return model, vocab, converter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e35b7721-0b98-417b-a382-29fc07618341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_infix_to_polish(model, vocab, converter, infix_expr: str, max_length: int = 15):\n",
    "    \"\"\"Convert infix expression to Polish notation using trained model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        infix_tokens = vocab.tokenize(infix_expr)\n",
    "        if len(infix_tokens) == 0:\n",
    "            return \"ERROR: Empty input\"\n",
    "            \n",
    "        src = torch.tensor([vocab.encode(infix_tokens)])\n",
    "        tgt = torch.tensor([[vocab.sos_id]])\n",
    "        \n",
    "        generated_tokens = []\n",
    "        \n",
    "        for step in range(max_length):\n",
    "            if tgt.size(1) > 20:  # Prevent infinite loops\n",
    "                break\n",
    "                \n",
    "            tgt_mask = model.generate_square_subsequent_mask(tgt.size(1))\n",
    "            \n",
    "            try:\n",
    "                output = model(src, tgt, tgt_mask=tgt_mask)\n",
    "                logits = output[0, -1, :]\n",
    "                \n",
    "                # Use top-k sampling for better results\n",
    "                top_k = 5\n",
    "                top_logits, top_indices = torch.topk(logits, top_k)\n",
    "                probs = F.softmax(top_logits, dim=-1)\n",
    "                next_token_idx = torch.multinomial(probs, 1).item()\n",
    "                next_token = top_indices[next_token_idx].item()\n",
    "                \n",
    "                if next_token == vocab.eos_id:\n",
    "                    break\n",
    "                if next_token == vocab.pad_id:\n",
    "                    continue\n",
    "                    \n",
    "                generated_tokens.append(next_token)\n",
    "                tgt = torch.cat([tgt, torch.tensor([[next_token]])], dim=1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                break\n",
    "        \n",
    "        # Decode result\n",
    "        result_tokens = [vocab.id_to_token[token_id] for token_id in generated_tokens \n",
    "                        if token_id in vocab.id_to_token]\n",
    "        \n",
    "        return ' '.join(result_tokens) if result_tokens else \"ERROR: No output\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2b4d6b57-52c6-4d02-8615-98581eef9d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Infix to Polish Notation Transformer...\n",
      "Initializing components...\n",
      "vocab: ['<PAD>', '<SOS>', '<EOS>', '<UNK>', '+', '-', '*', '/', '^', '(', ')', 'a', 'b', 'c']\n",
      "Vocabulary size: 14\n",
      "Sample vocab: ['<PAD>', '<SOS>', '<EOS>', '<UNK>', '+', '-', '*', '/', '^', '(', ')', 'a', 'b', 'c']\n",
      "Test conversion: b+c*a -> ['b', '+', 'c', '*', 'a'] -> ['+', 'b', '*', 'c', 'a']\n",
      "Generating training data...\n",
      "Example 1: (a*b)+c\n",
      "  Infix: ['(', 'a', '*', 'b', ')', '+', 'c'] -> [9, 11, 6, 12, 10, 4, 13]\n",
      "  Polish: ['+', '*', 'a', 'b', 'c'] -> [1, 4, 6, 11, 12, 13, 2]\n",
      "Example 2: c+a+a\n",
      "  Infix: ['c', '+', 'a', '+', 'a'] -> [13, 4, 11, 4, 11]\n",
      "  Polish: ['+', 'c', '+', 'a', 'a'] -> [1, 4, 13, 4, 11, 11, 2]\n",
      "Example 3: (c-c)+b\n",
      "  Infix: ['(', 'c', '-', 'c', ')', '+', 'b'] -> [9, 13, 5, 13, 10, 4, 12]\n",
      "  Polish: ['+', '-', 'c', 'c', 'b'] -> [1, 4, 5, 13, 13, 12, 2]\n",
      "Example 4: a+a\n",
      "  Infix: ['a', '+', 'a'] -> [11, 4, 11]\n",
      "  Polish: ['+', 'a', 'a'] -> [1, 4, 11, 11, 2]\n",
      "Example 5: (b-b)+c\n",
      "  Infix: ['(', 'b', '-', 'b', ')', '+', 'c'] -> [9, 12, 5, 12, 10, 4, 13]\n",
      "  Polish: ['+', '-', 'b', 'b', 'c'] -> [1, 4, 5, 12, 12, 13, 2]\n",
      "Generated 3000 valid training pairs\n",
      "Starting training...\n",
      "Epoch  1, Avg Loss: 0.5894, Batches: 188\n",
      "Epoch  2, Avg Loss: 0.1289, Batches: 188\n",
      "Epoch  3, Avg Loss: 0.0723, Batches: 188\n",
      "Epoch  4, Avg Loss: 0.0656, Batches: 188\n",
      "Epoch  5, Avg Loss: 0.0563, Batches: 188\n",
      "Epoch  6, Avg Loss: 0.0482, Batches: 188\n",
      "  Test: a+b -> Pred: '+ + a b b' | Actual: '+ a b'\n",
      "Epoch  7, Avg Loss: 0.0405, Batches: 188\n",
      "Epoch  8, Avg Loss: 0.0460, Batches: 188\n",
      "Epoch  9, Avg Loss: 0.0320, Batches: 188\n",
      "Epoch 10, Avg Loss: 0.0434, Batches: 188\n",
      "Epoch 11, Avg Loss: 0.0358, Batches: 188\n",
      "  Test: a+b -> Pred: '+ + a a' | Actual: '+ a b'\n",
      "Epoch 12, Avg Loss: 0.0253, Batches: 188\n",
      "Epoch 13, Avg Loss: 0.0510, Batches: 188\n",
      "Epoch 14, Avg Loss: 0.0259, Batches: 188\n",
      "Epoch 15, Avg Loss: 0.0290, Batches: 188\n",
      "Epoch 16, Avg Loss: 0.0201, Batches: 188\n",
      "  Test: a+b -> Pred: '+ + a b b' | Actual: '+ a b'\n",
      "Epoch 17, Avg Loss: 0.0512, Batches: 188\n",
      "Epoch 18, Avg Loss: 0.0244, Batches: 188\n",
      "Epoch 19, Avg Loss: 0.0333, Batches: 188\n",
      "Epoch 20, Avg Loss: 0.0263, Batches: 188\n",
      "Epoch 21, Avg Loss: 0.0055, Batches: 188\n",
      "  Test: a+b -> Pred: '+ a b' | Actual: '+ a b'\n",
      "Epoch 22, Avg Loss: 0.0059, Batches: 188\n",
      "Epoch 23, Avg Loss: 0.0112, Batches: 188\n",
      "Epoch 24, Avg Loss: 0.0039, Batches: 188\n",
      "Epoch 25, Avg Loss: 0.0031, Batches: 188\n",
      "\n",
      "Testing conversions:\n",
      "Infix: a+b\n",
      "Predicted: + a b\n",
      "Actual:    + a b\n",
      "Match: True\n",
      "----------------------------------------\n",
      "Infix: a+b*c\n",
      "Predicted: + a * b c\n",
      "Actual:    + a * b c\n",
      "Match: True\n",
      "----------------------------------------\n",
      "Infix: (a+b)*c\n",
      "Predicted: * + a b c\n",
      "Actual:    * + a b c\n",
      "Match: True\n",
      "----------------------------------------\n",
      "Infix: a*b+c\n",
      "Predicted: + * a b c\n",
      "Actual:    + * a b c\n",
      "Match: True\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Training Infix to Polish Notation Transformer...\")\n",
    "    model, vocab, converter = train_model()\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"Training failed!\")\n",
    "    else:\n",
    "        print(\"\\nTesting conversions:\")\n",
    "        test_expressions = [\"a+b\", \"a+b*c\", \"(a+b)*c\", \"a*b+c\"]\n",
    "        \n",
    "        for expr in test_expressions:\n",
    "            try:\n",
    "                predicted = convert_infix_to_polish(model, vocab, converter, expr)\n",
    "                actual_tokens = converter.infix_to_polish(vocab.tokenize(expr))\n",
    "                actual = ' '.join(actual_tokens)\n",
    "                \n",
    "                print(f\"Infix: {expr}\")\n",
    "                print(f\"Predicted: {predicted}\")\n",
    "                print(f\"Actual:    {actual}\")\n",
    "                print(f\"Match: {predicted.strip() == actual.strip()}\")\n",
    "                print(\"-\" * 40)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {expr}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
